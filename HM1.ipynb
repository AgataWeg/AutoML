{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM1 algorithms and hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# 1. data and alg choosement\n",
    "# 2. data preparation\n",
    "# 3. GS for each alg\n",
    "# 4. average model creation - default hyperparams selection\n",
    "\n",
    "# 5. Bayes Optimization\n",
    "# - in each iteration check the results and compare it with 'defaults'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms \n",
    "\n",
    "SVM\n",
    "\n",
    "NN\n",
    "\n",
    "Random forest\n",
    "\n",
    "# Data\n",
    "\n",
    "44 spam\n",
    "\n",
    "1067 nasa https://www.openml.org/search?type=data&status=active&id=1067\n",
    "\n",
    "1464 blood-transfusion-service-center\n",
    "\n",
    "40701 churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AWeg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AWeg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AWeg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\AWeg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "data = {}\n",
    "labels = {}\n",
    "id = [44,1067,1464,40701]\n",
    "for index in id:\n",
    "    dataset[index] = openml.datasets.get_dataset(index)\n",
    "    data[index], y, _, _ = dataset[index].get_data(dataset_format=\"dataframe\")\n",
    "    data[index] = data[index].replace('nan', np.nan) # not always needed\n",
    "    labels[index] = list(data[index].columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer, make_column_selector, ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    num_pipeline = Pipeline(steps=[\n",
    "        ('scale',MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    ### operacje dla kolumn kategorycznych\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        ('one-hot',OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "\n",
    "    ])\n",
    "\n",
    "    bool_pipeline = Pipeline(steps=[])\n",
    "\n",
    "    #### W funkcji ColumnTransformer podajemy liste transformerow dla poszczegolnych zestawow kolumn- (name, transformer, columns) \n",
    "    #### UWAGA: zamiast column selector (make_column_selector( dtype_include= np.number)) mozna podac wektor nazw kolumn, ale to rozwiazanie dla konkretnych danych\n",
    "\n",
    "\n",
    "    col_trans = ColumnTransformer(transformers=[\n",
    "        ('num_pipeline', num_pipeline, make_column_selector( dtype_exclude= np.object_)),\n",
    "        ('cat_pipeline',cat_pipeline,make_column_selector( dtype_include= np.object_)),\n",
    "        # ('bool_pipeline',bool_pipeline,make_column_selector( dtype_include= np.bool_)),\n",
    "        ],\n",
    "    \n",
    "        n_jobs=-1)\n",
    "    \n",
    "    model = Pipeline([('preprocessing', col_trans)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "1067\n",
      "1464\n",
      "40701\n"
     ]
    }
   ],
   "source": [
    "data_prepared = {}\n",
    "for index in id:\n",
    "    datad = data[index]\n",
    "    model =  preprocess()\n",
    "    model.get_params()\n",
    "    model.fit(datad)\n",
    "    after_process = model.transform(datad)\n",
    "    data_prepared[index] = pd.DataFrame(after_process, columns = labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "yColumnName = {44:\"class\",1067:\"defects\",1464:\"Class\",40701:\"class\"}\n",
    "X = {}\n",
    "y = {}\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "for index in id:\n",
    "\n",
    "    X[index] = data_prepared[index].drop(yColumnName[index], axis=1)\n",
    "    y[index] = data_prepared[index][yColumnName[index]]\n",
    "\n",
    "    X_train[index],  X_test[index], y_train[index], y_test[index] = train_test_split(X[index], y[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comment:\n",
    "crossvalidation(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for tuning hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('RandomForestClassifier', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('RandomForestClassifier', RandomForestClassifier())],\n",
       " 'verbose': False,\n",
       " 'RandomForestClassifier': RandomForestClassifier(),\n",
       " 'RandomForestClassifier__bootstrap': True,\n",
       " 'RandomForestClassifier__ccp_alpha': 0.0,\n",
       " 'RandomForestClassifier__class_weight': None,\n",
       " 'RandomForestClassifier__criterion': 'gini',\n",
       " 'RandomForestClassifier__max_depth': None,\n",
       " 'RandomForestClassifier__max_features': 'sqrt',\n",
       " 'RandomForestClassifier__max_leaf_nodes': None,\n",
       " 'RandomForestClassifier__max_samples': None,\n",
       " 'RandomForestClassifier__min_impurity_decrease': 0.0,\n",
       " 'RandomForestClassifier__min_samples_leaf': 1,\n",
       " 'RandomForestClassifier__min_samples_split': 2,\n",
       " 'RandomForestClassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'RandomForestClassifier__n_estimators': 100,\n",
       " 'RandomForestClassifier__n_jobs': None,\n",
       " 'RandomForestClassifier__oob_score': False,\n",
       " 'RandomForestClassifier__random_state': None,\n",
       " 'RandomForestClassifier__verbose': 0,\n",
       " 'RandomForestClassifier__warm_start': False}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = [{'RandomForestClassifier__n_estimators':np.linspace(10, 100, 10).astype(int),\n",
    "                'RandomForestClassifier__min_samples_leaf': np.linspace(1, 2, 1).astype(int),\n",
    "                'RandomForestClassifier__random_state': [1]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = []\n",
    "score_for_best_hyperparams = {}\n",
    "\n",
    "for index in id:\n",
    "    gs_model_pipeline = GridSearchCV(estimator=pipeline,param_grid=grid_params,scoring='roc_auc')\n",
    "    gs_model_pipeline.fit(X_train[index], y_train[index])\n",
    "    best_hyperparams.append(gs_model_pipeline.best_params_)\n",
    "\n",
    "    # score for test data\n",
    "    score_for_best_hyperparams[index] = gs_model_pipeline.score(X_test[index], y_test[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = []\n",
    "\n",
    "for index in range(0,4):\n",
    "    my_list.append( list(best_hyperparams[index].values()))\n",
    "\n",
    "keys = best_hyperparams[0].keys()\n",
    "\n",
    "mean_hyperparams = np.mean(np.array((my_list)),axis=0) # change it!\n",
    "\n",
    "mean_hyperparams = [ int(x) for x in mean_hyperparams ]\n",
    "keys = [ x.replace(\"RandomForestClassifier__\",\"\") for x in keys ]\n",
    "\n",
    "defaults = dict(zip(keys, mean_hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 1, 'n_estimators': 52, 'random_state': 1}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff for defaults - best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{44: -0.03548873791079321,\n",
       " 1067: 0.010059570059570033,\n",
       " 1464: 0.04377184142515811,\n",
       " 40701: 0.040565261028010724}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_for_default_hyperparams = {}\n",
    "diff = {}\n",
    "\n",
    "for index in id:\n",
    "    RF = RandomForestClassifier(min_samples_leaf = mean_hyperparams[0], n_estimators= mean_hyperparams[1], random_state= mean_hyperparams[2]) # change it in the future! \n",
    "    RF.fit(X_train[index], y_train[index])\n",
    "    score_for_default_hyperparams[index] = RF.score(X_test[index], y_test[index])\n",
    "    diff[index] = score_for_default_hyperparams[index] - score_for_best_hyperparams[index]\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "### definition of model\n",
    "\n",
    "# params_init = np.array([[-0.9], [1.1]])\n",
    "# Y_init = f(X_init)\n",
    "\n",
    "def rf_to_opt(params,X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    num_pipeline = Pipeline(steps=[\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('scale',MinMaxScaler())])\n",
    "\n",
    "    cat_pipeline = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "        ('one-hot',OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "\n",
    "    col_trans = ColumnTransformer(transformers=[\n",
    "    ('num_pipeline',num_pipeline, make_column_selector( dtype_include= np.number)),\n",
    "    ('cat_pipeline',cat_pipeline,make_column_selector( dtype_include= np.object_))\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=-1)\n",
    "    model_pipeline = Pipeline([('preprocessing', col_trans), \n",
    "                               ('model', RandomForestClassifier(n_estimators=np.int64(params[0])))])\n",
    "\n",
    "    \n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    return model_pipeline.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# rf_to_opt(params=[50])\n",
    "\n",
    "\n",
    "bounds = np.array([[5, 100]])\n",
    "params_init = np.array([[np.int64(5)], [np.int64(20)]])\n",
    "print(params_init)\n",
    "acc_init = np.array([rf_to_opt(params=params_init[0]),rf_to_opt(params=params_init[1]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from skopt import gp_minimize\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\n",
    "from bayesian_optimization_util import plot_approximation, plot_acquisition, plot_convergence\n",
    "\n",
    "np.int = np.int_\n",
    "\n",
    "\n",
    "noise = 0.2\n",
    "# Use custom kernel and estimator to match previous example\n",
    "\n",
    "m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "r = gp_minimize(lambda x: -rf_to_opt(params=x), \n",
    "                bounds.tolist(),\n",
    "                base_estimator=gpr,\n",
    "                acq_func='EI',      # expected improvement\n",
    "                xi=0.01,            # exploitation-exploration trade-off\n",
    "                n_calls=10,         # number of iterations\n",
    "                n_random_starts=0,  # initial samples are provided\n",
    "                x0=params_init.tolist(), # initial samples\n",
    "                y0=-acc_init.ravel()\n",
    "                )\n",
    "\n",
    "# print(r)\n",
    "\n",
    "## Fit GP model to samples for plotting results\n",
    "gpr.fit(r.x_iters, -r.func_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(np.array(r.x_iters), -r.func_vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
